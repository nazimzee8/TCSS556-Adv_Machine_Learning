{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cartpole-DeepQLearning2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUEkfrb1zma6",
        "outputId": "fe293ad8-13ce-4cb5-df83-291f6fccc7a4"
      },
      "source": [
        "import numpy as np #for our Qtable\r\n",
        "import gym #for our cartpole Environment\r\n",
        "import random #to generate random numbers\r\n",
        "import pandas\r\n",
        "from collections import deque\r\n",
        "\r\n",
        "#neural network packages\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.python.keras import utils\r\n",
        "\r\n",
        "#code for rendering gui\r\n",
        "!apt-get install python-opengl -y\r\n",
        "!apt install xvfb -y\r\n",
        "!pip install pyvirtualdisplay\r\n",
        "!pip install pyglet==1.4.0\r\n",
        "!apt-get install x11-utils\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from IPython import display as ipythondisplay\r\n",
        "from pyvirtualdisplay import Display\r\n",
        "display = Display(visible=0, size=(1400, 900))\r\n",
        "display.start()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 13 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 1s (561 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 146374 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 13 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 784 kB in 1s (875 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 148729 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/05/6568620fed440941b704664b9cfe5f836ad699ac7694745e7787fbdc8063/PyVirtualDisplay-2.0-py2.py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.0\n",
            "Collecting pyglet==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/2e/74069cfb668afcb29f0c7777c863d0b1d831accf61558f46cebf34bcfe07/pyglet-1.4.0-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.4.0) (0.16.0)\n",
            "Installing collected packages: pyglet\n",
            "  Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "Successfully installed pyglet-1.4.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libxxf86dga1\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libxxf86dga1 x11-utils\n",
            "0 upgraded, 2 newly installed, 0 to remove and 13 not upgraded.\n",
            "Need to get 209 kB of archives.\n",
            "After this operation, 711 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Fetched 209 kB in 1s (240 kB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "(Reading database ... 148736 files and directories currently installed.)\n",
            "Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+3build1) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Setting up x11-utils (7.7+3build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f3b37286d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq53yzr8zthh"
      },
      "source": [
        "class DQN():\r\n",
        "  def __init__(self,state_space,action_space,weights_file):\r\n",
        "    self.learning_rate = 0.2          # Learning rate\r\n",
        "    self.gamma = 0.95                 # Discounting rate\r\n",
        "\r\n",
        "    # Exploration parameters\r\n",
        "    self.epsilon = 0.99                 # Exploration rate\r\n",
        "    self.max_epsilon = 0.99             # Exploration probability at start\r\n",
        "    self.min_epsilon = 0.1            # Minimum exploration probability \r\n",
        "    self.decay_rate = 0.995            # Exponential decay rate for exploration prob\r\n",
        "\r\n",
        "    #neural network parameters\r\n",
        "    self.state_space = state_space\r\n",
        "    self.action_space=action_space\r\n",
        "    self.batch_size =16\r\n",
        "    self.input_size = state_space.shape[0] #or 4\r\n",
        "    self.output_size = action_space.n\r\n",
        "    self.model = self.buildQNetwork()\r\n",
        "    self.target = self.buildQNetwork()\r\n",
        "    self.memory = deque(maxlen=2000)  #doubel ended queue for storing the transitions\r\n",
        "    self.target_update_interval = 4\r\n",
        "    self.weights_file = weights_file\r\n",
        "\r\n",
        "  def memorize(self, state, action, reward, next_state, done):\r\n",
        "        self.memory.append((state, action, reward, next_state, done))\r\n",
        "  \r\n",
        "  def updateTarget(self,steps):\r\n",
        "    if steps >= self.batch_size and steps % self.target_update_interval == 0:\r\n",
        "      self.target.set_weights(self.model.get_weights())\r\n",
        "      print(\"target updated\")\r\n",
        "\r\n",
        "  def load_weights(self):\r\n",
        "    self.model.load_weights(self.weights_file)\r\n",
        "\r\n",
        "  def save_weights(self):\r\n",
        "    print(\"Saving wights to file\")\r\n",
        "    self.model.save_weights(self.weights_file)\r\n",
        "\r\n",
        "  \r\n",
        "  def buildQNetwork(self):\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Dense(10, input_dim=self.input_size, activation='relu'))#fully connected\r\n",
        "    model.add(Dense(10, activation='relu'))\r\n",
        "    model.add(Dense(self.output_size))\r\n",
        "    model.compile(loss='mse', optimizer='adam')\r\n",
        "    return model\r\n",
        "    \r\n",
        "  def chooseAction(self,state,isEpsilonGreedy):\r\n",
        "    exp_exp_tradeoff = random.uniform(0, 1)\r\n",
        "    \r\n",
        "    if exp_exp_tradeoff > self.epsilon or not isEpsilonGreedy:\r\n",
        "        qval = self.model.predict(np.reshape(state,(1, self.input_size)))[0]\r\n",
        "        maxqval = max(qval)\r\n",
        "        action= np.where(qval == maxqval)[0][0]\r\n",
        "        print(\"Qaction\",action)\r\n",
        "       \r\n",
        "    else:\r\n",
        "        action = self.action_space.sample()\r\n",
        "        print(\"random action\",action)\r\n",
        "    return action\r\n",
        "\r\n",
        "  def updateEpsilon(self,steps):\r\n",
        "    self.epsilon *= self.decay_rate\r\n",
        "    if(self.epsilon < self.min_epsilon):\r\n",
        "      self.epsilon = self.min_epsilon\r\n",
        "    #print(self.epsilon)\r\n",
        "    #self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon)*np.exp(-self.decay_rate*steps) \r\n",
        "\r\n",
        "  def calcualteTargetValue(self,state, action, reward, next_state, isDone):\r\n",
        "    qnext = self.target.predict(np.reshape(next_state,(1, self.input_size)))\r\n",
        "    maxqval = max(qnext[0])\r\n",
        "    \r\n",
        "   \r\n",
        "    if done:\r\n",
        "      target_value = reward\r\n",
        "    else:\r\n",
        "      target_value = reward + self.gamma *maxqval \r\n",
        "\r\n",
        "    return target_value\r\n",
        "\r\n",
        "  def train(self):\r\n",
        "    X_train = np.zeros((self.batch_size, self.input_size))\r\n",
        "    Y_train = np.zeros((self.batch_size, self.output_size))\r\n",
        "    loss =0\r\n",
        "    if len(self.memory)<self.batch_size:\r\n",
        "      print(\"memory insufficient for training\")\r\n",
        "      return loss\r\n",
        "    mini_batch = random.sample(self.memory, self.batch_size)\r\n",
        "    for index_rep in range(self.batch_size):\r\n",
        "      state,action,reward,next_state,isDone = mini_batch[index_rep]\r\n",
        "      X_train[index_rep] = state\r\n",
        "      Y_train[index_rep][action] = self.calcualteTargetValue(state,action,reward,next_state,isDone)\r\n",
        "    loss = self.model.train_on_batch(X_train, Y_train)\r\n",
        "    return loss"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "QdFxxnubz6Ny",
        "outputId": "22574bc2-9b4b-4943-fed1-7ff1b786f4de"
      },
      "source": [
        "env = gym.make(\"CartPole-v1\")\r\n",
        "\r\n",
        "rewards = []\r\n",
        "weights_file = 'dqn.h5'\r\n",
        "learner = DQN(env.observation_space,env.action_space,weights_file)\r\n",
        "total_episodes = 1000      # Total episodes\r\n",
        "max_steps = 500               # Max steps per episode\r\n",
        "total_steps=0 \r\n",
        "for episode in range(total_episodes): \r\n",
        "    # Reset the environment\r\n",
        "    state= env.reset()\r\n",
        "  \r\n",
        "    \r\n",
        "    step = 0\r\n",
        "    done = False\r\n",
        "    total_rewards = 0\r\n",
        "    loss =0\r\n",
        "    prev_screen = env.render(mode='rgb_array')\r\n",
        "    plt.imshow(prev_screen)\r\n",
        "    for step in range(max_steps): \r\n",
        "      screen = env.render(mode='rgb_array')\r\n",
        "      plt.imshow(screen)\r\n",
        "      ipythondisplay.clear_output(wait=True)\r\n",
        "      ipythondisplay.display(plt.gcf())\r\n",
        "    \r\n",
        "      \r\n",
        "      action = learner.chooseAction(state,True)\r\n",
        "      # Take the action (a) and observe the outcome state(s') and reward (r)\r\n",
        "      new_state, reward, done, info = env.step(action)\r\n",
        "\r\n",
        "      \r\n",
        "\r\n",
        "      learner.memorize(state,action,reward, new_state,done)\r\n",
        "      #Calcualte the loss and train (optimize) the Q-network\r\n",
        "      \r\n",
        "      loss +=learner.train()\r\n",
        "      learner.updateTarget(total_steps)\r\n",
        "\r\n",
        "      total_rewards += reward\r\n",
        "      total_steps += 1\r\n",
        "      # Our new state is state\r\n",
        "      state = new_state\r\n",
        "      \r\n",
        "      # If done (if we're dead) : finish episode\r\n",
        "      if done == True: \r\n",
        "          break\r\n",
        "  # Reduce epsilon (because we need less and less exploration)\r\n",
        "      learner.updateEpsilon(total_steps)\r\n",
        "    print(\"loss=\",loss)\r\n",
        "    rewards.append(total_rewards)\r\n",
        "    ipythondisplay.clear_output(wait=True)\r\n",
        "print(rewards, max(rewards))\r\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\r\n",
        "learner.save_weights()\r\n",
        "\r\n",
        "env.reset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "random action 1\n",
            "target updated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeeHWM0tz-II"
      },
      "source": [
        "for episode in range(5):\r\n",
        "    state = env.reset()\r\n",
        "   \r\n",
        "   \r\n",
        "    step = 0\r\n",
        "    done = False\r\n",
        "    print(\"****************************************************\")\r\n",
        "    print(\"EPISODE \", episode)\r\n",
        "    prev_screen = env.render(mode='rgb_array')\r\n",
        "    plt.imshow(prev_screen)\r\n",
        "    for step in range(max_steps):\r\n",
        "        \r\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\r\n",
        "        action = learner.chooseAction(state,False)\r\n",
        "        \r\n",
        "        new_state, reward, done, info = env.step(action)\r\n",
        "      \r\n",
        "       \r\n",
        "        screen = env.render(mode='rgb_array')\r\n",
        "        plt.imshow(screen)\r\n",
        "        ipythondisplay.clear_output(wait=True)\r\n",
        "        ipythondisplay.display(plt.gcf())\r\n",
        "        if done:\r\n",
        "          break\r\n",
        "        state = new_state\r\n",
        "    ipythondisplay.clear_output(wait=True)\r\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}