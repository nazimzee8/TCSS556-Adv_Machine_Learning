{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPole-DeepQLearning1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zv3TYQ1LJRB",
        "outputId": "045819f3-44ee-4b90-8a71-79b577b528e0"
      },
      "source": [
        "import numpy as np #for our Qtable\r\n",
        "import gym #for our cartpole Environment\r\n",
        "import random #to generate random numbers\r\n",
        "import pandas\r\n",
        "from collections import deque\r\n",
        "\r\n",
        "#neural network packages\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.python.keras import utils\r\n",
        "\r\n",
        "#code for rendering gui\r\n",
        "!apt-get install python-opengl -y\r\n",
        "!apt install xvfb -y\r\n",
        "!pip install pyvirtualdisplay\r\n",
        "!pip install pyglet==1.4.0\r\n",
        "!apt-get install x11-utils\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from IPython import display as ipythondisplay\r\n",
        "from pyvirtualdisplay import Display\r\n",
        "display = Display(visible=0, size=(1400, 900))\r\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (2.0)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Requirement already satisfied: pyglet==1.4.0 in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.4.0) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f7601d3ffd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liaaWlmcLKXU"
      },
      "source": [
        "#discreization parameters\r\n",
        "n_bins = 8\r\n",
        "n_bins_angle = 10\r\n",
        "# Number of states is huge so in order to simplify the situation\r\n",
        "# we discretize the space to: 10 ** number_of_features\r\n",
        "cart_position_bins = pandas.cut([-2.4, 2.4], bins=n_bins, retbins=True)[1][1:-1]\r\n",
        "pole_angle_bins = pandas.cut([-2, 2], bins=n_bins_angle, retbins=True)[1][1:-1]\r\n",
        "cart_velocity_bins = pandas.cut([-1, 1], bins=n_bins, retbins=True)[1][1:-1]\r\n",
        "angle_rate_bins = pandas.cut([-3.5, 3.5], bins=n_bins_angle, retbins=True)[1][1:-1]\r\n",
        "\r\n",
        "def to_bin(value, bins):\r\n",
        "    return np.digitize(x=[value], bins=bins)[0]\r\n",
        "\r\n",
        "def build_state(features):\r\n",
        "    return int(\"\".join(map(lambda feature: str(int(feature)), features)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSwgG1MfLYwc"
      },
      "source": [
        "class DQN():\r\n",
        "  def __init__(self,state_space,action_space,weights_file):\r\n",
        "    self.learning_rate = 0.2          # Learning rate\r\n",
        "    self.gamma = 0.95                 # Discounting rate\r\n",
        "\r\n",
        "    # Exploration parameters\r\n",
        "    self.epsilon = 0.99                 # Exploration rate\r\n",
        "    self.max_epsilon = 0.99             # Exploration probability at start\r\n",
        "    self.min_epsilon = 0.1            # Minimum exploration probability \r\n",
        "    self.decay_rate = 0.995            # Exponential decay rate for exploration prob\r\n",
        "\r\n",
        "    #neural network parameters\r\n",
        "    self.state_space = state_space\r\n",
        "    self.action_space=action_space\r\n",
        "    self.batch_size =16\r\n",
        "    self.input_size = 1\r\n",
        "    self.output_size = action_space.n\r\n",
        "    self.model = self.buildQNetwork()\r\n",
        "    self.target = self.buildQNetwork()\r\n",
        "    self.memory = deque(maxlen=2000)  #doubel ended queue for storing the transitions\r\n",
        "    self.target_update_interval = 4\r\n",
        "    self.weights_file = weights_file\r\n",
        "\r\n",
        "  def memorize(self, state, action, reward, next_state, done):\r\n",
        "        self.memory.append((state, action, reward, next_state, done))\r\n",
        "  \r\n",
        "  def updateTarget(self,steps):\r\n",
        "    if steps >= self.batch_size and steps % self.target_update_interval == 0:\r\n",
        "      self.target.set_weights(self.model.get_weights())\r\n",
        "      print(\"target updated\")\r\n",
        "\r\n",
        "  def load_weights(self):\r\n",
        "    self.model.load_weights(self.weights_file)\r\n",
        "\r\n",
        "  def save_weights(self):\r\n",
        "    print(\"Saving wights to file\")\r\n",
        "    self.model.save_weights(self.weights_file)\r\n",
        "\r\n",
        "  \r\n",
        "  def buildQNetwork(self):\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Dense(10, input_dim=self.input_size, activation='relu'))#fully connected\r\n",
        "    model.add(Dense(10, activation='relu'))\r\n",
        "    model.add(Dense(self.output_size))\r\n",
        "    model.compile(loss='mse', optimizer='adam')\r\n",
        "    return model\r\n",
        "    \r\n",
        "  def chooseAction(self,state,isEpsilonGreedy):\r\n",
        "    exp_exp_tradeoff = random.uniform(0, 1)\r\n",
        "    \r\n",
        "    if exp_exp_tradeoff > self.epsilon or not isEpsilonGreedy:\r\n",
        "        qval = self.model.predict(np.reshape(state,(1, self.input_size)))[0]\r\n",
        "        maxqval = max(qval)\r\n",
        "        action= np.where(qval == maxqval)[0][0]\r\n",
        "        print(\"Qaction\",action)\r\n",
        "       \r\n",
        "    else:\r\n",
        "        action = self.action_space.sample()\r\n",
        "        print(\"random action\",action)\r\n",
        "    return action\r\n",
        "\r\n",
        "  def updateEpsilon(self,steps):\r\n",
        "    self.epsilon *= self.decay_rate\r\n",
        "    if(self.epsilon < self.min_epsilon):\r\n",
        "      self.epsilon = self.min_epsilon\r\n",
        "    #print(self.epsilon)\r\n",
        "    #self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon)*np.exp(-self.decay_rate*steps) \r\n",
        "\r\n",
        "  def calcualteTargetValue(self,state, action, reward, next_state, isDone):\r\n",
        "    qnext = self.target.predict(np.reshape(next_state,(1, self.input_size)))\r\n",
        "    maxqval = max(qnext[0])\r\n",
        "   \r\n",
        "    if done:\r\n",
        "      target_value = reward\r\n",
        "    else:\r\n",
        "      target_value = reward + self.gamma *maxqval \r\n",
        "\r\n",
        "    return target_value\r\n",
        "\r\n",
        "  def train(self):\r\n",
        "    X_train = np.zeros((self.batch_size, self.input_size))\r\n",
        "    Y_train = np.zeros((self.batch_size, self.output_size))\r\n",
        "    loss =0\r\n",
        "    if len(self.memory)<self.batch_size:\r\n",
        "      print(\"memory insufficient for training\")\r\n",
        "      return loss\r\n",
        "    mini_batch = random.sample(self.memory, self.batch_size)\r\n",
        "    for index_rep in range(self.batch_size):\r\n",
        "      state,action,reward,next_state,isDone = mini_batch[index_rep]\r\n",
        "      X_train[index_rep] = state\r\n",
        "      Y_train[index_rep][action] = self.calcualteTargetValue(state,action,reward,next_state,isDone)\r\n",
        "    loss = self.model.train_on_batch(X_train, Y_train)\r\n",
        "    return loss\r\n",
        "\r\n",
        "\r\n",
        "env = gym.make(\"CartPole-v1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duV3Ky31Mj62"
      },
      "source": [
        "rewards = []\r\n",
        "weights_file = 'dqn.h5'\r\n",
        "learner = DQN(env.observation_space,env.action_space,weights_file)\r\n",
        "total_episodes = 1000      # Total episodes\r\n",
        "max_steps = 500               # Max steps per episode\r\n",
        "total_steps=0 \r\n",
        "for episode in range(total_episodes): \r\n",
        "    # Reset the environment\r\n",
        "    cart_position, pole_angle, cart_velocity, angle_rate_of_change = env.reset()\r\n",
        "  \r\n",
        "    state = build_state([to_bin(cart_position, cart_position_bins),\r\n",
        "                        to_bin(pole_angle, pole_angle_bins),\r\n",
        "                        to_bin(cart_velocity, cart_velocity_bins),\r\n",
        "                        to_bin(angle_rate_of_change, angle_rate_bins)])\r\n",
        "    step = 0\r\n",
        "    done = False\r\n",
        "    total_rewards = 0\r\n",
        "    loss =0\r\n",
        "    prev_screen = env.render(mode='rgb_array')\r\n",
        "    plt.imshow(prev_screen)\r\n",
        "    for step in range(max_steps): \r\n",
        "      screen = env.render(mode='rgb_array')\r\n",
        "      plt.imshow(screen)\r\n",
        "      ipythondisplay.clear_output(wait=True)\r\n",
        "      ipythondisplay.display(plt.gcf())\r\n",
        "    \r\n",
        "      \r\n",
        "      action = learner.chooseAction(state,True)\r\n",
        "      # Take the action (a) and observe the outcome state(s') and reward (r)\r\n",
        "      observation, reward, done, info = env.step(action)\r\n",
        "\r\n",
        "      cart_position, pole_angle, cart_velocity, angle_rate_of_change = observation\r\n",
        "      new_state = build_state([to_bin(cart_position, cart_position_bins),\r\n",
        "                        to_bin(pole_angle, pole_angle_bins),\r\n",
        "                        to_bin(cart_velocity, cart_velocity_bins),\r\n",
        "                        to_bin(angle_rate_of_change, angle_rate_bins)])\r\n",
        "\r\n",
        "      learner.memorize(state,action,reward, new_state,done)\r\n",
        "      #Calcualte the loss and train (optimize) the Q-network\r\n",
        "      \r\n",
        "      loss +=learner.train()\r\n",
        "      learner.updateTarget(total_steps)\r\n",
        "\r\n",
        "      total_rewards += reward\r\n",
        "      total_steps += 1\r\n",
        "      # Our new state is state\r\n",
        "      state = new_state\r\n",
        "      \r\n",
        "      # If done (if we're dead) : finish episode\r\n",
        "      if done == True: \r\n",
        "          break\r\n",
        "  # Reduce epsilon (because we need less and less exploration)\r\n",
        "      learner.updateEpsilon(total_steps)\r\n",
        "    print(\"loss=\",loss)\r\n",
        "    rewards.append(total_rewards)\r\n",
        "    ipythondisplay.clear_output(wait=True)\r\n",
        "print(rewards, max(rewards))\r\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\r\n",
        "learner.save_weights()\r\n",
        "\r\n",
        "env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ36RkMuMnyU"
      },
      "source": [
        "for episode in range(5):\r\n",
        "    cart_position, pole_angle, cart_velocity, angle_rate_of_change = env.reset()\r\n",
        "   \r\n",
        "    state = build_state([to_bin(cart_position, cart_position_bins),\r\n",
        "                         to_bin(pole_angle, pole_angle_bins),\r\n",
        "                         to_bin(cart_velocity, cart_velocity_bins),\r\n",
        "                         to_bin(angle_rate_of_change, angle_rate_bins)])\r\n",
        "    step = 0\r\n",
        "    done = False\r\n",
        "    print(\"****************************************************\")\r\n",
        "    print(\"EPISODE \", episode)\r\n",
        "    prev_screen = env.render(mode='rgb_array')\r\n",
        "    plt.imshow(prev_screen)\r\n",
        "    for step in range(max_steps):\r\n",
        "        \r\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\r\n",
        "        action = learner.chooseAction(state,False)\r\n",
        "        \r\n",
        "        observation, reward, done, info = env.step(action)\r\n",
        "        cart_position, pole_angle, cart_velocity, angle_rate_of_change = observation\r\n",
        "        new_state = build_state([to_bin(cart_position, cart_position_bins),\r\n",
        "                          to_bin(pole_angle, pole_angle_bins),\r\n",
        "                          to_bin(cart_velocity, cart_velocity_bins),\r\n",
        "                          to_bin(angle_rate_of_change, angle_rate_bins)])\r\n",
        "        screen = env.render(mode='rgb_array')\r\n",
        "        plt.imshow(screen)\r\n",
        "        ipythondisplay.clear_output(wait=True)\r\n",
        "        ipythondisplay.display(plt.gcf())\r\n",
        "        if done:\r\n",
        "          break\r\n",
        "        state = new_state\r\n",
        "    ipythondisplay.clear_output(wait=True)\r\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}