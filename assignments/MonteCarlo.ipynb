{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MonteCarlo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOC5-ATb8VTh",
        "outputId": "9871f5a7-2fbc-4d2b-b2cf-f45f857f7547"
      },
      "source": [
        "import numpy as np #for our Qtable\r\n",
        "import gym #for our FrozenLake Environment\r\n",
        "import random #to generate random numbers\r\n",
        "\r\n",
        "env = gym.make(\"FrozenLake-v0\")\r\n",
        "\r\n",
        "action_size = env.action_space.n\r\n",
        "state_size = env.observation_space.n\r\n",
        "print(action_size, state_size)\r\n",
        "\r\n",
        "returns_sum = np.zeros((state_size, action_size))\r\n",
        "#qtable = np.zeros((state_size, action_size))\r\n",
        "#print(qtable)\r\n",
        "\r\n",
        "total_episodes = 40000       # Total episodes\r\n",
        "max_steps = 99               # Max steps per episode\r\n",
        "learning_rate = 0.2          # Learning rate             # Max steps per episode\r\n",
        "gamma = 0.95                 # Discounting rate\r\n",
        " \r\n",
        "# Exploration parameters\r\n",
        "epsilon = 1.0                 # Exploration rate\r\n",
        "max_epsilon = 1.0             # Exploration probability at start\r\n",
        "min_epsilon = 0.01            # Minimum exploration probability \r\n",
        "decay_rate = 0.005            # Exponential decay rate for explorationprob\r\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUNDAtnr8bQY",
        "outputId": "018198c1-4bfe-4a62-b1ff-c383199c37c6"
      },
      "source": [
        "rewards = []\r\n",
        "visited = []\r\n",
        "N = np.zeros((state_size, action_size))\r\n",
        "for i in range(total_episodes):\r\n",
        "  state = env.reset()\r\n",
        "  done = False\r\n",
        "  total_rewards = 0\r\n",
        "  for t in range(max_steps):\r\n",
        "    exp_exp_tradeoff = random.uniform(0, 1)\r\n",
        "    if exp_exp_tradeoff > epsilon: #select an action with the maximum q-value\r\n",
        "        action = np.argmax(qtable[state,:])\r\n",
        "        #print(exp_exp_tradeoff, \"action\", action)\r\n",
        "    else:\r\n",
        "        action = env.action_space.sample()\r\n",
        "    new_state, reward, done, info = env.step(action)\r\n",
        "\r\n",
        "    if (state, action) not in visited:\r\n",
        "      my_tuple = (state, action)\r\n",
        "      visited.append(my_tuple)\r\n",
        "      N[state, action] += 1\r\n",
        "      #returns_sum[state, action] += reward\r\n",
        "    total_rewards += reward\r\n",
        "    qtable[state, action] = total_rewards/N[state, action]\r\n",
        "    state = new_state\r\n",
        "    if done == True: \r\n",
        "      break\r\n",
        "\r\n",
        "  # Reduce epsilon (because we need less and less exploration)\r\n",
        "  epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*i) \r\n",
        "  rewards.append(total_rewards)\r\n",
        "print(\"total score: \" + str(sum(rewards)/total_episodes))\r\n",
        "#print(qtable)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total score: 7.5e-05\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}