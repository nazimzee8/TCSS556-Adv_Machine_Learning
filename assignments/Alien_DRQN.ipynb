{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alien_DRQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v7ljxG9MDNe",
        "outputId": "8dd9f93b-7268-4cf6-ea5b-dbee8bd40a06"
      },
      "source": [
        "import numpy as np #for our Qtable\r\n",
        "import gym #for our cartpole Environment\r\n",
        "import random #to generate random numbers\r\n",
        "import pandas\r\n",
        "from collections import deque\r\n",
        "\r\n",
        "#neural network packages\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D,Flatten,TimeDistributed,LSTM\r\n",
        "from tensorflow.python.keras import utils\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "#image processing opencv\r\n",
        "import cv2\r\n",
        "\r\n",
        "#code for rendering gui\r\n",
        "!apt-get install python-opengl -y\r\n",
        "!apt install xvfb -y\r\n",
        "!pip install pyvirtualdisplay\r\n",
        "!pip install pyglet==1.4.0\r\n",
        "!apt-get install x11-utils\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from IPython import display as ipythondisplay\r\n",
        "from pyvirtualdisplay import Display\r\n",
        "display = Display(visible=0, size=(1800, 1800))\r\n",
        "display.start()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (2.0)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Requirement already satisfied: pyglet==1.4.0 in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.4.0) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fcadf95f5c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp4VHx3VMI49"
      },
      "source": [
        "class ReplayBuffer():\r\n",
        "  def __init__(self, maxlength, historylength):\r\n",
        "    self.maxlength = maxlength\r\n",
        "    self.historylength = historylength\r\n",
        "    self.action = None\r\n",
        "    self.observation = None\r\n",
        "    self.reward = None\r\n",
        "    self.done = None\r\n",
        "    self.nextindex =-1\r\n",
        "    self.totalentries =0\r\n",
        "  \r\n",
        "  def getSize(self):\r\n",
        "    return self.totalentries\r\n",
        "\r\n",
        "  def appendFrame(self,state):\r\n",
        "    if self.observation is None:\r\n",
        "      self.action = np.empty([self.maxlength])\r\n",
        "      self.observation = np.empty([self.maxlength] + list(state.shape),dtype=np.float32)\r\n",
        "      self.reward = np.empty([self.maxlength])\r\n",
        "      self.done = np.empty([self.maxlength])\r\n",
        "\r\n",
        "   \r\n",
        "   \r\n",
        "    self.nextindex = (self.nextindex + 1) % self.maxlength\r\n",
        "    self.observation[self.nextindex] = state\r\n",
        "    self.totalentries = min(self.maxlength, self.totalentries + 1)\r\n",
        "  \r\n",
        "  def appendEffect(self, action, reward,done):\r\n",
        "    self.reward[self.nextindex] = reward\r\n",
        "    self.action[self.nextindex] = action\r\n",
        "    self.done[self.nextindex] = done\r\n",
        "\r\n",
        "  def getBatch(self, idxes):\r\n",
        "    obs_batch      = np.concatenate([self.concatFrames(idx)[np.newaxis, :] for idx in idxes], axis=0)\r\n",
        "    act_batch      = self.action[idxes]\r\n",
        "    rew_batch      = self.reward[idxes]\r\n",
        "    next_obs_batch = np.concatenate([self.concatFrames(idx + 1)[np.newaxis, :] for idx in idxes], axis=0)\r\n",
        "    done_batch      = self.done[idxes]\r\n",
        "    return obs_batch, act_batch, rew_batch, next_obs_batch, done_batch\r\n",
        "\r\n",
        "  def sample(self, batch_size):\r\n",
        "    idxes=random.sample(range(0, self.totalentries-2), batch_size)\r\n",
        "    return self.getBatch(idxes)\r\n",
        "\r\n",
        "  def get_recent_state(self):\r\n",
        "    return self.concatFrames((self.nextindex-1) % self.maxlength)\r\n",
        "  \r\n",
        "\r\n",
        "  def concatFrames(self,idx):\r\n",
        "    end_idx   = idx + 1 # make noninclusive\r\n",
        "    start_idx = end_idx - self.historylength\r\n",
        "    \r\n",
        "    #low dimensional observations\r\n",
        "    if len(self.observation.shape) == 2:\r\n",
        "        return self.observation[end_idx-1]\r\n",
        "\r\n",
        "    # insufficient frames in buffer\r\n",
        "    if start_idx < 0 and self.totalentries != self.maxlength:\r\n",
        "        start_idx = 0\r\n",
        "    for idx in range(start_idx, end_idx - 1):\r\n",
        "        if self.done[idx % self.maxlength]:\r\n",
        "            start_idx = idx + 1\r\n",
        "    img_h, img_w = self.observation.shape[1], self.observation.shape[2]     \r\n",
        "    #fill the missing history frames with zeros\r\n",
        "    missing_context = self.historylength - (end_idx - start_idx)\r\n",
        "    if start_idx < 0 or missing_context > 0:\r\n",
        "        frames = [np.zeros_like(self.observation[0]) for _ in range(missing_context)]\r\n",
        "        for idx in range(start_idx, end_idx):\r\n",
        "            frames.append(self.observation[idx % self.maxlength])\r\n",
        "        return np.reshape(frames,(self.historylength,img_h, img_w,1))\r\n",
        "    else:\r\n",
        "        \r\n",
        "        return self.observation[start_idx:end_idx].reshape(self.historylength,img_h, img_w,1)\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV-NTwLrML6M"
      },
      "source": [
        "class DQN():\r\n",
        "  def __init__(self,state_space,action_space,weights_file):\r\n",
        "    self.learning_rate = 0.2          # Learning rate\r\n",
        "    self.gamma = 0.95                 # Discounting rate\r\n",
        "\r\n",
        "    # Exploration parameters\r\n",
        "    self.epsilon = 0.99                 # Exploration rate\r\n",
        "    self.max_epsilon = 0.99             # Exploration probability at start\r\n",
        "    self.min_epsilon = 0.1            # Minimum exploration probability \r\n",
        "    self.decay_rate = 0.995            # Exponential decay rate for exploration prob\r\n",
        "\r\n",
        "    #neural network parameters\r\n",
        "    self.state_space = state_space\r\n",
        "    self.action_space=action_space\r\n",
        "    self.batch_size =16\r\n",
        "    self.input_size = state_space.shape\r\n",
        "    self.output_size = action_space.n\r\n",
        "    self.model = self.buildQNetwork()\r\n",
        "    self.target = self.buildQNetwork()\r\n",
        "    #self.memory = deque(maxlen=2000)  #doubel ended queue for storing the transitions\r\n",
        "    self.historylength=4\r\n",
        "    self.memory = ReplayBuffer(2000,self.historylength)\r\n",
        "    self.target_update_interval = 4\r\n",
        "    self.weights_file = weights_file\r\n",
        "    self.image_width =84\r\n",
        "    self.image_height =84\r\n",
        "    self.channels=1\r\n",
        "\r\n",
        "  def addFrame(self, state):\r\n",
        "    self.memory.appendFrame(state)\r\n",
        "\r\n",
        "  def addEffect(self, action, reward, done):\r\n",
        "    self.memory.appendEffect(action, reward, done)\r\n",
        "  \r\n",
        "  def getRecentState(self):\r\n",
        "    return self.memory.get_recent_state()\r\n",
        "\r\n",
        "  def resetBuffer(self):\r\n",
        "    self.memory = ReplayBuffer(2000,self.historylength)\r\n",
        "    \r\n",
        "  def updateTarget(self,steps):\r\n",
        "    if self.memory.getSize() >= 2*self.batch_size and steps % self.target_update_interval == 0:\r\n",
        "      self.target.set_weights(self.model.get_weights())\r\n",
        "      print(\"target updated\")\r\n",
        "\r\n",
        "  def load_weights(self):\r\n",
        "    self.model.load_weights(self.weights_file)\r\n",
        "\r\n",
        "  def save_weights(self):\r\n",
        "    print(\"Saving wights to file\")\r\n",
        "    self.model.save_weights(self.weights_file)\r\n",
        "\r\n",
        "  def preprocess(self,image):\r\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\r\n",
        "\r\n",
        "    image = cv2.resize(image, (self.image_width, self.image_height), interpolation=cv2.INTER_AREA)\r\n",
        "   \r\n",
        "    return  np.reshape(image,(self.image_width, self.image_height,1))\r\n",
        "  \r\n",
        "  \r\n",
        "  def buildQNetwork(self):\r\n",
        "    model = Sequential()\r\n",
        "    model.add(TimeDistributed(Conv2D(32, (3,3), input_shape=(4,84,84,1), activation='relu')))\r\n",
        "    model.add(TimeDistributed(MaxPool2D(pool_size=(2,2))))\r\n",
        "    model.add(TimeDistributed(Conv2D(64, (3,3),activation='relu')))\r\n",
        "    model.add(TimeDistributed(MaxPool2D(pool_size=(2,2))))\r\n",
        "    model.add(TimeDistributed(Flatten()))\r\n",
        "    model.add(LSTM(512))\r\n",
        "    model.add(Dense(10, activation='relu'))#fully connected\r\n",
        "    model.add(Dense(10, activation='relu'))\r\n",
        "    model.add(Dense(self.output_size))\r\n",
        "    model.compile(loss='mse', optimizer=Adam(lr=0.0001, clipvalue=1))\r\n",
        "    return model\r\n",
        "\r\n",
        "  def predict(self, state, isTarget):\r\n",
        "    state = np.reshape(state, (1,) + state.shape)\r\n",
        "    state = tf.cast(state,dtype=tf.float32)\r\n",
        "    if isTarget:\r\n",
        "      result= self.target.predict(state)\r\n",
        "    else:\r\n",
        "      result= self.model.predict(state)\r\n",
        "    #print(result.shape)\r\n",
        "    return result\r\n",
        "\r\n",
        "    \r\n",
        "  def chooseAction(self,state,isEpsilonGreedy):\r\n",
        "    exp_exp_tradeoff = random.uniform(0, 1)\r\n",
        "    \r\n",
        "    if exp_exp_tradeoff > self.epsilon or not isEpsilonGreedy:\r\n",
        "        qval = self.predict(state,False)[0]\r\n",
        "        maxqval = max(qval)\r\n",
        "        action= np.where(qval == maxqval)[0][0]\r\n",
        "        print(\"Qaction\",action)\r\n",
        "       \r\n",
        "    else:\r\n",
        "        action = self.action_space.sample()\r\n",
        "        print(\"random action\",action)\r\n",
        "    return action\r\n",
        "\r\n",
        "  def updateEpsilon(self,steps):\r\n",
        "    self.epsilon *= self.decay_rate\r\n",
        "    if(self.epsilon < self.min_epsilon):\r\n",
        "      self.epsilon = self.min_epsilon\r\n",
        "    #print(self.epsilon)\r\n",
        "    #self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon)*np.exp(-self.decay_rate*steps) \r\n",
        "\r\n",
        "  def calcualteTargetValue(self,state, action, reward, next_state, isDone):\r\n",
        "    \r\n",
        "    \r\n",
        "    qnext = self.predict(next_state, True)\r\n",
        "    maxqval = max(qnext[0])\r\n",
        "   \r\n",
        "    if done:\r\n",
        "      target_value = reward\r\n",
        "    else:\r\n",
        "      target_value = reward + self.gamma *maxqval \r\n",
        "\r\n",
        "    return target_value\r\n",
        "\r\n",
        "  def train(self):\r\n",
        "    X_train = np.zeros((self.batch_size,self.historylength,self.image_width,self.image_height,self.channels))\r\n",
        "    Y_train = np.zeros((self.batch_size, self.output_size))\r\n",
        "    loss =0\r\n",
        "    if self.memory.getSize()< 2*self.batch_size:\r\n",
        "      print(\"memory insufficient for training\")\r\n",
        "      return loss\r\n",
        "\r\n",
        "    state_batch, act_batch, rew_batch, next_state_batch, done_batch = self.memory.sample(self.batch_size)\r\n",
        "    for index_rep in range(self.batch_size):\r\n",
        "      X_train[index_rep] = state_batch[index_rep]\r\n",
        "      Y_train[index_rep][action] = self.calcualteTargetValue(state_batch[index_rep],act_batch[index_rep],rew_batch[index_rep],next_state_batch[index_rep],done_batch[index_rep])\r\n",
        "    #print(\"train shape\",X_train.shape,\"yshape\",Y_train.shape)\r\n",
        "    loss = self.model.train_on_batch(X_train, Y_train)\r\n",
        "    \r\n",
        "    return loss"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "e-xdduGqMQaZ",
        "outputId": "71ffe70d-cf61-4288-9777-f005ab754844"
      },
      "source": [
        "env = gym.make(\"Alien-v0\")\r\n",
        "#print(env.observation_space,env.observation_space.shape)\r\n",
        "#print(env.action_space)\r\n",
        "rewards = []\r\n",
        "weights_file = 'dqn.h5'\r\n",
        "learner = DQN(env.observation_space,env.action_space,weights_file)\r\n",
        "total_episodes = 2      # Total episodes\r\n",
        "max_steps = 50              # Max steps per episode\r\n",
        "total_steps=0\r\n",
        "skipFrames = 3\r\n",
        "for episode in range(total_episodes): \r\n",
        "    # Reset the environment\r\n",
        "    state = env.reset()\r\n",
        "    \r\n",
        "\r\n",
        "    step = 0\r\n",
        "    done = False\r\n",
        "    total_rewards = 0\r\n",
        "    loss =0\r\n",
        "    prev_screen = env.render(mode='rgb_array')\r\n",
        "    plt.imshow(prev_screen)\r\n",
        "    for step in range(max_steps): \r\n",
        "      screen = env.render(mode='rgb_array')\r\n",
        "      plt.imshow(screen)\r\n",
        "      ipythondisplay.clear_output(wait=True)\r\n",
        "      ipythondisplay.display(plt.gcf())\r\n",
        "      if(total_steps % skipFrames == 0):\r\n",
        "        state = learner.preprocess(state)\r\n",
        "        learner.addFrame(state)\r\n",
        "\r\n",
        "      action = learner.chooseAction(learner.getRecentState(),True)\r\n",
        "      # Take the action (a) and observe the outcome state(s') and reward (r)\r\n",
        "      new_state, reward, done, info = env.step(action)\r\n",
        "\r\n",
        "      if(total_steps % skipFrames == 0):\r\n",
        "        learner.addEffect(action,reward,done)\r\n",
        "      #Calcualte the loss and train (optimize) the Q-network\r\n",
        "      \r\n",
        "      if(total_steps % skipFrames == 0):\r\n",
        "        print(\"updating Q-network\")\r\n",
        "        loss +=learner.train()\r\n",
        "      \r\n",
        "      learner.updateTarget(int(total_steps/skipFrames))\r\n",
        "\r\n",
        "      total_rewards += reward\r\n",
        "      total_steps += 1\r\n",
        "      # Our new state is state\r\n",
        "      state = new_state\r\n",
        "      \r\n",
        "      # If done (if we're dead) : finish episode\r\n",
        "      if done == True:\r\n",
        "        print(\"game over\")\r\n",
        "        break\r\n",
        "  # Reduce epsilon (because we need less and less exploration)\r\n",
        "      learner.updateEpsilon(total_steps)\r\n",
        "    print(\"loss=\",loss)\r\n",
        "    rewards.append(total_rewards)\r\n",
        "    ipythondisplay.clear_output(wait=True)\r\n",
        "print(rewards, max(rewards))\r\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\r\n",
        "learner.save_weights()\r\n",
        "\r\n",
        "\r\n",
        "env.reset()\r\n",
        "learner.epsilon = learner.min_epsilon"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqFAbeGoHcZ3"
      },
      "source": [
        "for episode in range(5):\r\n",
        "    state = env.reset()\r\n",
        "   \r\n",
        "    \r\n",
        "    step = 0\r\n",
        "    done = False\r\n",
        "    print(\"****************************************************\")\r\n",
        "    print(\"EPISODE \", episode)\r\n",
        "    prev_screen = env.render(mode='rgb_array')\r\n",
        "    plt.imshow(prev_screen)\r\n",
        "\r\n",
        "    \r\n",
        "    learner.resetBuffer()\r\n",
        "    for step in range(max_steps):\r\n",
        "        \r\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\r\n",
        "\r\n",
        "        state = learner.preprocess(state)\r\n",
        "\r\n",
        "        learner.addFrame(state)\r\n",
        "        action = learner.chooseAction(learner.getRecentState(),True)\r\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\r\n",
        "        new_state, reward, done, info = env.step(action)\r\n",
        "\r\n",
        "      \r\n",
        "        learner.addEffect(action,reward,done)\r\n",
        "\r\n",
        "        \r\n",
        "        screen = env.render(mode='rgb_array')\r\n",
        "        plt.imshow(screen)\r\n",
        "        ipythondisplay.clear_output(wait=True)\r\n",
        "        ipythondisplay.display(plt.gcf())\r\n",
        "        if done:\r\n",
        "          break\r\n",
        "        state = new_state\r\n",
        "    ipythondisplay.clear_output(wait=True)\r\n",
        "env.close()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}